# -*- coding: utf-8 -*-
"""Assignment_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wg5kf9oJUZ4unG_hLTYRGUHLXslJighX

Task 1
"""

# specify the data file name and url
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/'
datafile = url + 'UCI%20HAR%20Dataset.zip'

# download the zip file from the web server using curl
!curl $datafile --output UCI_HAR_Dataset.zip

# unzip the file
!unzip -qq UCI_HAR_Dataset.zip

# change the directory name to remove spaces
!mv -f UCI\HAR\Dataset UCI_HAR_DATASET

# Delete everything
#!rm -rf UCI*
#!rm -rf _*
#!rm -rf sample*

"""Our dataset consist of observations for 6 activities:

*   Walking
*   Walking upstairs
*   Walking downstairs
*   Sitting
*   Standing
*   Laying

For each activity we measure:

*   Triaxial acceleration from the accelerometer and the estimated body acceleration
*   Triaxial Angular velocity from the gyroscope.
*   A 561 feature vector with time and frequency domain variables
*   Its a activity label
*   An identifier of the subject carried out the experiment. (30 persons total)

More precicely.
y is our response variables (7352x1) result of 7352 observation
x is our covariances (7352x561). 7352 observations with 561 features each

Task 2
"""

#Read data from file and reset display format to display full numbers
import pandas as pd
#X=pd.read_csv("./UCI HAR Dataset/test/X_test.txt",header=None,delim_whitespace=True)
pd.options.display.float_format = '{:,.10f}'.format

# Walking
# From y_test we choose all the walking observations
# From all the walking observations we chose a random element
# For that element we print a plot with the three lines x,y,z coordinates

temp=pd.read_csv("./UCI HAR Dataset/test/y_test.txt",header=None)
import matplotlib.pyplot as plt

# For walking chose one line with value 1.
while True:
  y=temp.sample(1,replace=False)
  if y.iloc[0,0]==1:
    break
temp_acc_x=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt",delim_whitespace=True,header=None)
temp_acc_y=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt",delim_whitespace=True,header=None)
temp_acc_z=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt",delim_whitespace=True,header=None)

plt.plot(temp_acc_x.columns,temp_acc_x.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_y.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_z.loc[y.index.values[0]])
plt.legend(["x","y","z"])
plt.title("Walking")

# For sitting chose one line with value 4.
while True:
  y=temp.sample(1,replace=False)
  if y.iloc[0,0]==4:
    break
temp_acc_x=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt",delim_whitespace=True,header=None)
temp_acc_y=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt",delim_whitespace=True,header=None)
temp_acc_z=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt",delim_whitespace=True,header=None)

plt.plot(temp_acc_x.columns,temp_acc_x.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_y.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_z.loc[y.index.values[0]])
plt.legend(["x","y","z"])
plt.title("Sitting")

# For standing chose one line with value 5.
while True:
  y=temp.sample(1,replace=False)
  if y.iloc[0,0]==5:
    break
temp_acc_x=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt",delim_whitespace=True,header=None)
temp_acc_y=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt",delim_whitespace=True,header=None)
temp_acc_z=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt",delim_whitespace=True,header=None)

plt.plot(temp_acc_x.columns,temp_acc_x.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_y.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_z.loc[y.index.values[0]])
plt.legend(["x","y","z"])
plt.title("Standing")

# For laying chose one line with value 6.
while True:
  y=temp.sample(1,replace=False)
  if y.iloc[0,0]==6:
    break
temp_acc_x=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt",delim_whitespace=True,header=None)
temp_acc_y=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt",delim_whitespace=True,header=None)
temp_acc_z=pd.read_csv("./UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt",delim_whitespace=True,header=None)

plt.plot(temp_acc_x.columns,temp_acc_x.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_y.loc[y.index.values[0]])
plt.plot(temp_acc_x.columns,temp_acc_z.loc[y.index.values[0]])
plt.legend(["x","y","z"])
plt.title("Laying")

"""Task 3"""

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

np.set_printoptions(suppress=True)

# Train
x_train=pd.read_csv("./UCI HAR Dataset/train/X_train.txt",delim_whitespace=True,header=None)
y_train=pd.read_csv("./UCI HAR Dataset/train/y_train.txt",delim_whitespace=True,header=None)
# Test
x_test=pd.read_csv("./UCI HAR Dataset/test/X_test.txt",delim_whitespace=True,header=None)
y_test=pd.read_csv("./UCI HAR Dataset/test/y_test.txt",delim_whitespace=True,header=None)

print ('x_train samples = ' + str(x_train.shape))
print ('y_train samples = ' + str(y_train.shape))

print ('x_test samples = ' + str(x_test.shape))
print ('y_test samples = ' + str(y_test.shape))

y_input=tf.keras.utils.to_categorical(y_train-1)
x_input=x_train.astype(np.float32)

print ('x_input samples = ' + str(x_input.shape))
print ('y_input samples = ' + str(y_input.shape))

x_input[0].dtype

W=tf.Variable(tf.zeros([561,6]))
b=tf.Variable(tf.zeros([6]))

def y_pred(x):
  return tf.nn.softmax(tf.matmul(x,W)+b)

@tf.function
def loss(x,y):
  y_=y_pred(x)
  return tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(tf.clip_by_value(y_,1e-10,1.0)),axis=[1]))

"""Task 4"""

def accuracy(x,y):
  y_=y_pred(x)
  correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
  accuracy=tf.reduce_mean(tf.cast(correct_prediction,"float"))
  return accuracy

logdir="tflogs1"
writer=tf.summary.create_file_writer(logdir)

train_steps=10000
lr=1e-1
optimizer=tf.optimizers.SGD(lr)

with writer.as_default():
  for i in range(train_steps):
    with tf.GradientTape() as tape:
      current_loss = loss(x_input,y_input)
    gradients = tape.gradient(current_loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W ,b]))
    tf.summary.scalar("loss",current_loss,step=i)
    if i%100 == 0:
        print('Training Step:' + str(i) + '  Loss = ' + str(current_loss))

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir tflogs1

# accuracy on training data
accuracy(x_input,y_input)

# accuracy on testing data
y_input_test=tf.keras.utils.to_categorical(y_test-1)
x_input_test=x_test.astype(np.float32)

accuracy(x_input_test,y_input_test)

"""Task 5"""

W=tf.Variable(tf.zeros([561,6]))
b=tf.Variable(tf.zeros([6]))

logdir="tflogs2"
writer=tf.summary.create_file_writer(logdir)

def y_pred(x):
  return tf.nn.softmax(tf.matmul(x,W)+b)

@tf.function
def loss(x,y):
  y_=y_pred(x)
  return tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(tf.clip_by_value(y_,1e-10,1.0)),axis=[1]))

# in the task it wasnt clear if we needed to run Adam with the default learning
# rate 0.001 or SGD learning rate 0.1. I explained what happened in both cases.
train_steps=10000
lr=0.001
optimizer=tf.keras.optimizers.Adam(learning_rate=lr)

with writer.as_default():
  for i in range(train_steps):
    with tf.GradientTape() as tape:
      current_loss = loss(x_input,y_input)
    gradients = tape.gradient(current_loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W ,b]))
    tf.summary.scalar("loss",current_loss,step=i)
    if i%100 == 0:
        print('Training Step:' + str(i) + '  Loss = ' + str(current_loss))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir tflogs2

# accuracy on training data
accuracy(x_input,y_input)

# accuracy on testing data
y_input_test=tf.keras.utils.to_categorical(y_test-1)
x_input_test=x_test.astype(np.float32)

accuracy(x_input_test,y_input_test)

"""Task 6

In task 5 it doesnt clarify if we want to run Adam optimizer with the same learning rate as SGD.

If we run Adam optimizer with the same learning rate 0.1 we notice that Adam optimizer does not converge. The loss function stabilize at 11.50 and it also has low accuracy.

Running Adam optimizer with the default learning rate 0.001 which is smaller than 0.1 we notice that Adam optimizer converge faster compared to SGD. Meaning the loss function for Adam optimizer get lower values compared to loss function to SGD with the same number of rounds.

Task 7

Our weights are in the tensor W (561x6)
"""

#abs

abs_W=tf.math.abs(W)
# For class 0 Walking
class_0_top_50=tf.math.top_k(abs_W[:,0],k=50)

# For class 1 Walking Upstairs
class_1_top_50=tf.math.top_k(abs_W[:,1],k=50)

# For class 2 Walking downstairs
class_2_top_50=tf.math.top_k(abs_W[:,2],k=50)

# For class 3 Sitting
class_3_top_50=tf.math.top_k(abs_W[:,3],k=50)

# For class 4 Standing
class_4_top_50=tf.math.top_k(abs_W[:,4],k=50)

# For class 5 Laying
class_5_top_50=tf.math.top_k(abs_W[:,5],k=50)

temp=tf.concat([class_0_top_50.indices,class_1_top_50.indices],axis=0)
temp=tf.concat([temp,class_2_top_50.indices],axis=0)
temp=tf.concat([temp,class_3_top_50.indices],axis=0)
temp=tf.concat([temp,class_4_top_50.indices],axis=0)
temp=tf.concat([temp,class_5_top_50.indices],axis=0)

# contains the indexes of the weights we care (unique from all lists)
unique_top_50=tf.unique(temp).y

# Our model
def Model(x):
  New_X=tf.gather(x_input,unique_top_50,axis=1)
  New_W=tf.gather(W,unique_top_50,axis=0)
  return tf.nn.softmax(tf.matmul(x,W)+b)

"""Task 8"""

W=tf.Variable(tf.zeros([561,6]))
b=tf.Variable(tf.zeros([6]))

logdir="tflogs3"
writer=tf.summary.create_file_writer(logdir)

def y_pred(x):
  return Model(x)

@tf.function
def loss(x,y):
  y_=y_pred(x)
  return tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(tf.clip_by_value(y_,1e-10,1.0)),axis=[1]))

train_steps=10000
lr=0.001
optimizer=tf.keras.optimizers.Adam(learning_rate=lr)

with writer.as_default():
  for i in range(train_steps):
    with tf.GradientTape() as tape:
      current_loss = loss(x_input,y_input)
    gradients = tape.gradient(current_loss, [W, b])
    optimizer.apply_gradients(zip(gradients, [W ,b]))
    tf.summary.scalar("loss",current_loss,step=i)
    if i%100 == 0:
        print('Training Step:' + str(i) + '  Loss = ' + str(current_loss))

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir tflogs3

# accuracy on training data
accuracy(x_input,y_input)

# accuracy on testing data
y_input_test=tf.keras.utils.to_categorical(y_test-1)
x_input_test=x_test.astype(np.float32)

accuracy(x_input_test,y_input_test)

"""As we can see if we run Adam with the default learning rate with all features and with the selected features we get similar loss value after 9900 rounds (0.014546951 vs 0.014546951).

That means that we can safely ignore alot of features and still get similar results. That way we can reduce our workload.

We get similar results if we run Adam with the SGD learning rate. Meaning, although in that case we dont see the loss function converge (It stabilarize around 11.5) we can safetly ignore it most features and still get the same results. That way we can reduce our workload
"""